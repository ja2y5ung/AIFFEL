{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premier-retro",
   "metadata": {},
   "source": [
    "# ‚úåüèª‚úäüèªüñêüèª Ïù∏Í≥µÏßÄÎä•Í≥º Í∞ÄÏúÑÎ∞îÏúÑÎ≥¥ ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-consortium",
   "metadata": {},
   "source": [
    "## 1. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interested-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL ÎùºÏù¥Î∏åÎü¨Î¶¨ import ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL ÎùºÏù¥Î∏åÎü¨Î¶¨ import ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "electronic-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "Í∞ÄÏúÑ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # ÌååÏùºÎßàÎã§ Î™®Îëê 28x28 ÏÇ¨Ïù¥Ï¶àÎ°ú Î∞îÍæ∏Ïñ¥ Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# Í∞ÄÏúÑ Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†ÄÏû•Îêú ÎîîÎ†âÌÜ†Î¶¨ ÏïÑÎûòÏùò Î™®Îì† jpg ÌååÏùºÏùÑ ÏùΩÏñ¥Îì§Ïó¨ÏÑú\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"Í∞ÄÏúÑ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-employer",
   "metadata": {},
   "source": [
    "### 1-2. Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ Î≥ÄÍ≤Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "headed-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "Ï£ºÎ®π Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path2 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path2)\n",
    "\n",
    "print(\"Ï£ºÎ®π Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aboriginal-dynamics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "Î≥¥ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path3 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path3)\n",
    "\n",
    "print(\"Î≥¥ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-blackberry",
   "metadata": {},
   "source": [
    "### 1-3. Îç∞Ïù¥ÌÑ∞ ÎùºÎ≤® ÏÉùÏÑ± Î∞è Ï†ÑÏ≤òÎ¶¨(Ï†ïÍ∑úÌôî)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "egyptian-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµÎç∞Ïù¥ÌÑ∞(x_train)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî 300 ÏûÖÎãàÎã§.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # Í∞ÄÏúÑÎ∞îÏúÑÎ≥¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò Ï¥ùÌï©Ïóê Ï£ºÏùòÌïòÏÑ∏Ïöî.\n",
    "    # Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏôÄ ÎùºÎ≤®(Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2) Îç∞Ïù¥ÌÑ∞Î•º Îã¥ÏùÑ ÌñâÎ†¨(matrix) ÏòÅÏó≠ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=0   # Í∞ÄÏúÑ : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=1   # Î∞îÏúÑ : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=2   # Î≥¥ : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"ÌïôÏäµÎç∞Ïù¥ÌÑ∞(x_train)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî\", idx,\"ÏûÖÎãàÎã§.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # ÏûÖÎ†•ÏùÄ 0~1 ÏÇ¨Ïù¥Ïùò Í∞íÏúºÎ°ú Ï†ïÍ∑úÌôî\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "heard-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÎùºÎ≤®:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print('ÎùºÎ≤®: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-cholesterol",
   "metadata": {},
   "source": [
    "## 2. Îî•Îü¨Îãù ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÑ§Í≥Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "national-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                51264     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 56,547\n",
      "Trainable params: 56,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handled-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28, 28, 3)\n",
      "(300,)\n",
      "(300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-journey",
   "metadata": {},
   "source": [
    "### 2-2. ÎÑ§Ìä∏ÏõåÌÅ¨ ÌïôÏäµ ÏãúÌÇ§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "moderate-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 6s 246ms/step - loss: 1.1078 - accuracy: 0.3713\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0475 - accuracy: 0.5433\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9754 - accuracy: 0.6227\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8600 - accuracy: 0.8161\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.7680\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5264 - accuracy: 0.9085\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.9260\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.9365\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2559 - accuracy: 0.9346\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb584e7fb90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-richmond",
   "metadata": {},
   "source": [
    "## 3. ÎÑ§Ìä∏ÏõåÌÅ¨ test Ìï¥Î≥¥Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-enterprise",
   "metadata": {},
   "source": [
    "### 3-1. test Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÌÅ¨Í∏∞ Î≥ÄÍ≤Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reported-crazy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images resized.\n",
      "ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path4 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "resize_images(image_dir_path4)\n",
    "\n",
    "print(\"ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "statutory-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aiffel/aiffel/rock_scissor_paper/test'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir_path4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-tampa",
   "metadata": {},
   "source": [
    "### 3-2. test Îç∞Ïù¥ÌÑ∞ ÎùºÎ≤® ÏÉùÏÑ± Î∞è Ï†ÑÏ≤òÎ¶¨(Ï†ïÍ∑úÌôî)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "japanese-editor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞(x_test)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî 300 ÏûÖÎãàÎã§.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data2(img_path, number_of_data=300):  # Í∞ÄÏúÑÎ∞îÏúÑÎ≥¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò Ï¥ùÌï©Ïóê Ï£ºÏùòÌïòÏÑ∏Ïöî.\n",
    "    # Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏôÄ ÎùºÎ≤®(Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2) Îç∞Ïù¥ÌÑ∞Î•º Îã¥ÏùÑ ÌñâÎ†¨(matrix) ÏòÅÏó≠ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor_*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=0   # Í∞ÄÏúÑ : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+ '/rock_*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=1   # Î∞îÏúÑ : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=2   # Î≥¥ : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞(x_test)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî\", idx,\"ÏûÖÎãàÎã§.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_test, y_test)=load_data2(image_dir_path4)\n",
    "x_test_norm = x_test/255.0   # ÏûÖÎ†•ÏùÄ 0~1 ÏÇ¨Ïù¥Ïùò Í∞íÏúºÎ°ú Ï†ïÍ∑úÌôî\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-wages",
   "metadata": {},
   "source": [
    "### 3-3. test Îç∞Ïù¥ÌÑ∞Î•º predict Ìï¥Î≥¥Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "honest-fortune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 3.2595 - accuracy: 0.3333\n",
      "test_loss: 3.2594995498657227 \n",
      "test_accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-telephone",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\"> ‚Äª Ïù∏ÏãùÎ•†Ïù¥ ÏÉùÍ∞ÅÎ≥¥Îã§ ÎÑàÎ¨¥ ÎÇÆÏùå. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-african",
   "metadata": {},
   "source": [
    "## 4. Îçî Ï¢ãÏùÄ ÎÑ§Ìä∏ÏõåÌÅ¨ ÎßåÎì§Ïñ¥Î≥¥Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-diving",
   "metadata": {},
   "source": [
    "### 4-1. Í∞úÏÑ† Î∞©Î≤ï Ï†úÏãú\n",
    ">     - ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞Í∞í Î≥ÄÍ≤Ω.\n",
    ">     - ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í∞ØÏàò ÎäòÎ¶º.\n",
    ">     - Ïò§Î≤Ñ ÌîºÌåÖ Î∞©ÏßÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-swing",
   "metadata": {},
   "source": [
    "### 4-2. ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞Í∞í Î≥ÄÍ≤Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "normal-louisiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 516)               132612    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1551      \n",
      "=================================================================\n",
      "Total params: 504,979\n",
      "Trainable params: 504,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(256, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(516, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unlimited-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 7s 382ms/step - loss: 1.1026 - accuracy: 0.3544\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0734 - accuracy: 0.4090\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8734 - accuracy: 0.7884\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.6133 - accuracy: 0.7033\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2477 - accuracy: 0.8864\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0646 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0050 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb57c20ea90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "weekly-greensboro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 6.9043 - accuracy: 0.4167\n",
      "test_loss: 6.904300212860107 \n",
      "test_accuracy: 0.4166666567325592\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-olympus",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> ‚Äª ÌÅ∞ Î≥ÄÌôîÎäî ÏóÜÎã§. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-shanghai",
   "metadata": {},
   "source": [
    "### 4-3. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í∞ØÏàò ÌôïÏû• + Ïò§Î≤ÑÌîºÌåÖ Î∞©ÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "recorded-living",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650  images to be resized.\n",
      "1650  images resized.\n",
      "Í∞ÄÏúÑ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n",
      "1650  images to be resized.\n",
      "1650  images resized.\n",
      "Ï£ºÎ®π Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n",
      "1650  images to be resized.\n",
      "1650  images resized.\n",
      "Î≥¥ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_2/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"Í∞ÄÏúÑ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")\n",
    "\n",
    "image_dir_path2 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_2/rock\"\n",
    "resize_images(image_dir_path2)\n",
    "print(\"Ï£ºÎ®π Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")\n",
    "\n",
    "image_dir_path3 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_2/paper\"\n",
    "resize_images(image_dir_path3)\n",
    "print(\"Î≥¥ Ïù¥ÎØ∏ÏßÄ resize ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "contained-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµÎç∞Ïù¥ÌÑ∞(x_train)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî 3600 ÏûÖÎãàÎã§.\n",
      "x_train shape: (4950, 28, 28, 3)\n",
      "y_train shape: (4950,)\n"
     ]
    }
   ],
   "source": [
    "def load_data3(img_path, number_of_data=4950):  # Í∞ÄÏúÑÎ∞îÏúÑÎ≥¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò Ï¥ùÌï©Ïóê Ï£ºÏùòÌïòÏÑ∏Ïöî.\n",
    "    # Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏôÄ ÎùºÎ≤®(Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2) Îç∞Ïù¥ÌÑ∞Î•º Îã¥ÏùÑ ÌñâÎ†¨(matrix) ÏòÅÏó≠ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=0   # Í∞ÄÏúÑ : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=1   # Î∞îÏúÑ : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=2   # Î≥¥ : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"ÌïôÏäµÎç∞Ïù¥ÌÑ∞(x_train)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî\", idx,\"ÏûÖÎãàÎã§.\")\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "image_dir_path4 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/new_train_data\"\n",
    "(x_train, y_train)=load_data3(image_dir_path4)\n",
    "x_train_norm = x_train/255.0   # ÏûÖÎ†•ÏùÄ 0~1 ÏÇ¨Ïù¥Ïùò Í∞íÏúºÎ°ú Ï†ïÍ∑úÌôî\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "continent-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 127,043\n",
      "Trainable params: 127,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "technological-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.9334 - accuracy: 0.5116\n",
      "Epoch 2/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.8079 - accuracy: 0.5014\n",
      "Epoch 3/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.7625 - accuracy: 0.5814\n",
      "Epoch 4/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.5482 - accuracy: 0.7367\n",
      "Epoch 5/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.4187 - accuracy: 0.8252\n",
      "Epoch 6/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.3237 - accuracy: 0.8732\n",
      "Epoch 7/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.2625 - accuracy: 0.8956\n",
      "Epoch 8/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1992 - accuracy: 0.9221\n",
      "Epoch 9/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1880 - accuracy: 0.9338\n",
      "Epoch 10/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1633 - accuracy: 0.9342\n",
      "Epoch 11/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.9408\n",
      "Epoch 12/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1739 - accuracy: 0.9355\n",
      "Epoch 13/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1040 - accuracy: 0.9692\n",
      "Epoch 14/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1094 - accuracy: 0.9607\n",
      "Epoch 15/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.0958 - accuracy: 0.9658\n",
      "Epoch 16/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.0711 - accuracy: 0.9736\n",
      "Epoch 17/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.1007 - accuracy: 0.9654\n",
      "Epoch 18/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.0653 - accuracy: 0.9742\n",
      "Epoch 19/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.0792 - accuracy: 0.9700\n",
      "Epoch 20/20\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 0.0524 - accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb57bed34d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "forbidden-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞(x_test)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî 1650 ÏûÖÎãàÎã§.\n",
      "x_test shape: (1650, 28, 28, 3)\n",
      "y_test shape: (1650,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data3(img_path, number_of_data=1650):  # Í∞ÄÏúÑÎ∞îÏúÑÎ≥¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò Ï¥ùÌï©Ïóê Ï£ºÏùòÌïòÏÑ∏Ïöî.\n",
    "    # Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏôÄ ÎùºÎ≤®(Í∞ÄÏúÑ : 0, Î∞îÏúÑ : 1, Î≥¥ : 2) Îç∞Ïù¥ÌÑ∞Î•º Îã¥ÏùÑ ÌñâÎ†¨(matrix) ÏòÅÏó≠ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor_*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=0   # Í∞ÄÏúÑ : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+ '/rock_*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=1   # Î∞îÏúÑ : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Ïóê Ïù¥ÎØ∏ÏßÄ ÌñâÎ†¨ÏùÑ Î≥µÏÇ¨\n",
    "        labels[idx]=2   # Î≥¥ : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞(x_test)Ïùò Ïù¥ÎØ∏ÏßÄ Í∞úÏàòÎäî\", idx,\"ÏûÖÎãàÎã§.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path5 = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_2/test\"\n",
    "(x_test, y_test)=load_data3(image_dir_path5)\n",
    "x_test_norm = x_test/255.0   # ÏûÖÎ†•ÏùÄ 0~1 ÏÇ¨Ïù¥Ïùò Í∞íÏúºÎ°ú Ï†ïÍ∑úÌôî\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "wrong-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 - 0s - loss: 1.3019 - accuracy: 0.8030\n",
      "test_loss: 1.3018699884414673 \n",
      "test_accuracy: 0.8030303120613098\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "attachments": {
    "asdf%20s.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "psychological-asthma",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> ‚Äª ÏµúÏ¢Ö Ïù∏ÏãùÎ•† : 0.803 </span>\n",
    "![asdf%20s.PNG](attachment:asdf%20s.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-doctrine",
   "metadata": {},
   "source": [
    "# üí° ÌöåÍ≥†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-experience",
   "metadata": {},
   "source": [
    "> #### - ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Í∞Ä ÎßéÏùÑÏàòÎ°ù Ïù∏ÏãùÎ•†Ïù¥ Ïò¨ÎùºÍ∞ê.\n",
    "> #### - ÏûÖÎ†•ÎêòÎäî Îç∞Ïù¥ÌÑ∞ÎßàÎã§ ÌäπÏßïÏù¥ Îã§Î¶Ñ.\n",
    "> #### - ÌïòÏù¥Ìçº ÌååÎùºÎØ∏ÌÑ∞ Í∞íÏùÑ Ïûò ÎßûÏ∂∞ÏïºÌï®.\n",
    "> #### - Ïò§Î≤ÑÌåÖÏùÑ Î∞©ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ÏÑú Dropout Î†àÏù¥Ïñ¥Î•º Ï∂îÍ∞ÄÌï®."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
