{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "terminal-antarctica",
   "metadata": {},
   "source": [
    "# 💻 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-atmosphere",
   "metadata": {},
   "source": [
    "## Step1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-silver",
   "metadata": {},
   "source": [
    "이번 프로젝트에 사용할 데이터는 LMS cloud ~/aiffel/lyricist/data/lyrics 경로에 있는 텍스트 데이터를 사용했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-wildlife",
   "metadata": {},
   "source": [
    "## Step2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-african",
   "metadata": {},
   "source": [
    "이번 프로젝트에 필요한 라이브러리 호출 및 데이터 정보를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-paraguay",
   "metadata": {},
   "source": [
    "## Step3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-morning",
   "metadata": {},
   "source": [
    "자연어 처리에서는 데이터가 알맞은 목적에 맞게 전처리가 돼있지 않은 상태라면, 용도에 맞게 토큰화(Tokenization) 및 정제(cleaning) 과정을 거친다.\n",
    "\n",
    " \"Time is an illusion. Lunchtime double so!\" 이라는 문장이 주어졌을때, 이 문장은 토큰화가 단순하다. 그러나 실제로 토큰화 작업을 수행하다 보면, 예상치 못한 경우가 생긴다.\n",
    " \n",
    " 사용 목적, 의도에 맞게 토큰화 하는 알고리즘을 짜야한다. 이번 프로젝트에서는 여러 특수 기호가 들어가지 않은 단어를 뽑아야 하기 때문에 특수문자, 공백, 대문자를 조절해 단어만 뽑는 알고리즘을 사용한다.\n",
    " \n",
    " \n",
    "> - 정제 : 갖고있는 데이터로부터 노이즈 데이터를 제거한다.\n",
    "> - 정규화 : 의미는 같지만 표현 방법이 다른 단어들을 통합시켜 같은 단어로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infinite-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence2 = sentence.split(' ')\n",
    "    if len(sentence2) > 13:\n",
    "        return 0 \n",
    "    else:\n",
    "        sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-hungarian",
   "metadata": {},
   "source": [
    "본인은 토큰화 했을 때 토큰의 갯수가 15개를 넘어가지 않는 문장 데이터를 권한다는 조건을 받았기에, <start>, <end>를 제외한 단어의 길이가 13개를 초과하지 않는 조건문을 걸었다.\n",
    "    \n",
    "   sentence가 13을 초과할경우 0을 반환하고 그렇지 않으면 sentence를 그대로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "future-saturn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if preprocessed_sentence == 0:\n",
    "        pass\n",
    "    else:\n",
    "        corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-harassment",
   "metadata": {},
   "source": [
    "이후에 정제된 텍스트 데이터를 모아놓은 preprocessed_sentence의 요소가 \"0\"(단어의 갯수가 13개 이상인 문장)일경우 지나치고 \"0\"이 아니라면 corpus에 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "discrete-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 271 ...   0   0   0]\n",
      " [  2 118   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  75  45 ...   3   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 633 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f95a4dc9e10>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-occupation",
   "metadata": {},
   "source": [
    "Tensorflow의 Tokenizer와 pad_sequences를 사용해 텍스트 데이터를 토큰화 한다.\n",
    "본인은 문장을 토큰화 했을때 15개를 초과하지 않으려고 pad_sequences의 maxlen인자로\n",
    "15의 값을 주었지만, 데이터를 제외시키는게 아닌 슬라이싱의 기능을 가지므로 maxlen을\n",
    "지워야 했다.\n",
    "\n",
    "단어장의 크기는 12000이상 이라는 조건이 있으므로, 12000개의 단어를 기억할수 있는 tokenizer를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "narrow-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  62 271  27  94 546  20  86 742  90   3   0   0   0]\n",
      "[ 62 271  27  94 546  20  86 742  90   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-mounting",
   "metadata": {},
   "source": [
    "tensor에서 각각의 마지막 토큰을 잘라내서 입력할 소스 문장을 생성한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "absent-isolation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-camping",
   "metadata": {},
   "source": [
    "## Step4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-chicken",
   "metadata": {},
   "source": [
    "sklearn 모듈의 train_test_split()함수를 이용해 학습데이터와 테스트 데이터를\n",
    "8:2 비율로 분리했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "monetary-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size = 0.2,\n",
    "                                                          random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "possible-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "formal-intellectual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-reply",
   "metadata": {},
   "source": [
    "## Step5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-centre",
   "metadata": {},
   "source": [
    "실습 내용에서 사용한 모델의 구조는 1개의 Embedding 레이어 2개의 LSTM 레이어, 1개의 Dense레이어로 구성되있다. 각 레이어의 기능은 다음과 같다.\n",
    "\n",
    "> - Embedding : 인간의 언어(자연어)는 수치화 되있지 않은 데이터기 때문에 바로 모델에 사용할 수 없다. 그래서 자연어 처리에서 특징 추출을 해줘야 하는데 이것이 \"언어의 벡터화\"이다. 이런 벡터화의 과정을 Word Embedding이라고 한다.\n",
    "> - LSTM : (Long Short Term Memory)의 약자로 기존의 RNN이 출력과 먼 위치에 있는 정보를 기억할 수 없다는 단점을 보완하여 장/단기 기억을 가능하게 설계한 신경망의 구조이다.\n",
    "> - Dense : 다층 퍼셉트론 신경망에서 사용되는 레이어로 입력과 출력을 모두 연결해준다.\n",
    "> - hidden layer : 인공 신경망의 히든 레이어는 입력 레이어와 출력 레이어 사이에 있는 레이어로, 여기서 인공 뉴런은 가중 입력 세트를 받아 활성화 함수를 통해 출력을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "parallel-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-priest",
   "metadata": {},
   "source": [
    "- embedding size를 256으로 주었다는건, 하나의 token을 256개의 벡터로 바꿔준다는 뜻이다. 나아가 모델 개선하기 파트에서 여러 파라미터 값을 주어 결과를 확인 해보도록 하자.\n",
    "\n",
    "- hidden size는 은닉 상태의 크기를 정의한다. RNN의 용량을 늘린다고 보면되며, 중소형 모델의 경우 보통 128, 256, 512, 1024등의 값을 갖는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dynamic-celebrity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 14, 12001])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)\n",
    "model(src_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "least-award",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-lottery",
   "metadata": {},
   "source": [
    "- optimizer란 머신러닝 학습에서 실제로 파라미터를 갱신시키는 부분을 의미한다. 이번 실습에서 사용할 optimizer는 Adam이다.\n",
    "  Adam은 현재 가장 자주 사용되는 optimizer로 폭넓은 딥러닝 아키텍쳐에서 좋은 성능을 보인다고 한다.\n",
    "  Adam말고도 cs231n에 소개한 Stochastic Gradient Descent(SGD), NAG, RMSProp, AdaGrad 등이 있다.\n",
    "  \n",
    "  \n",
    "\n",
    "- loss(손실 함수)란 컴퓨터는 처음에 랜덤값을 대입해 문제를 풀어본다. 결과와 실제 정답간의 간격 즉, 차이를 최대한 줄이는 방향으로\n",
    "  값을 대입하게된다. 이 값의 차이를 loss라고 한다. 이 loss를 줄이는 방향으로 학습이 진행된다. loss의 종류에는 \n",
    "  MSE, MRSE, Binary Crossentropy등이 있으며 이번 실습에 사용할 loss는 Categorical Crossentropy다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 157s 317ms/step - loss: 4.0284\n",
      "Epoch 2/10\n",
      "268/487 [===============>..............] - ETA: 1:10 - loss: 3.0834"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history = model.fit(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "copyrighted-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "established-instrument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-seeking",
   "metadata": {},
   "source": [
    "## Step5. 인공지능 모델 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-gardening",
   "metadata": {},
   "source": [
    "### (1) hidden_size 값 감소시켜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "suspended-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 72s 143ms/step - loss: 4.4185\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 3.3409\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 3.1391\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 3.0003\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 70s 142ms/step - loss: 2.9041\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 69s 142ms/step - loss: 2.8166\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 2.7474\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 2.6843\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 2.6311\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 70s 143ms/step - loss: 2.5674\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "model2 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model2.compile(loss=loss, optimizer=optimizer)\n",
    "history2 = model2.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "boxed-friendly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 43s 83ms/step - loss: 4.7102\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 3.4388\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 3.3071\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 3.1462\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 3.0386\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 2.9700\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 2.9009\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 41s 83ms/step - loss: 2.8401\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 41s 84ms/step - loss: 2.7789\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 41s 84ms/step - loss: 2.7238\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "model3 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model3.compile(loss=loss, optimizer=optimizer)\n",
    "history3 = model3.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "continuing-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 32s 60ms/step - loss: 5.0712\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 30s 61ms/step - loss: 3.5111\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 30s 61ms/step - loss: 3.3391\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 30s 61ms/step - loss: 3.1844\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 30s 61ms/step - loss: 3.0724\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 30s 61ms/step - loss: 2.9993\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 29s 60ms/step - loss: 2.9464\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 29s 60ms/step - loss: 2.8859\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 30s 61ms/step - loss: 2.8444\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 30s 60ms/step - loss: 2.8062\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 128\n",
    "model4 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model4.compile(loss=loss, optimizer=optimizer)\n",
    "history4 = model4.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-start",
   "metadata": {},
   "source": [
    "embedding_size를 그대로 두고 hidden_size만 감소 시켰더니 학습 시간을 줄지만 loss값은 대폭 증가했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-venture",
   "metadata": {},
   "source": [
    "### (2) embedding size 값 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-circumstances",
   "metadata": {},
   "source": [
    "imbedding size를 256으로 주었다는건, 하나의 token을 300개의 벡터로 바꿔준다는 뜻이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "powered-cherry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 155s 308ms/step - loss: 4.1067\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 153s 313ms/step - loss: 3.1095\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 153s 313ms/step - loss: 2.9488\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 153s 313ms/step - loss: 2.8248\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 153s 313ms/step - loss: 2.7208\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 153s 313ms/step - loss: 2.6295\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 153s 313ms/step - loss: 2.5450\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 152s 312ms/step - loss: 2.4647\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 151s 309ms/step - loss: 2.3891\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 152s 311ms/step - loss: 2.3211\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 1024\n",
    "model5 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model5.compile(loss=loss, optimizer=optimizer)\n",
    "history5 = model5.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "artificial-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 152s 302ms/step - loss: 4.1280\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 149s 305ms/step - loss: 3.1498\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 148s 304ms/step - loss: 2.9924\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 149s 305ms/step - loss: 2.8699\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 148s 304ms/step - loss: 2.7666\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 148s 304ms/step - loss: 2.6898\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 148s 304ms/step - loss: 2.6092\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 148s 304ms/step - loss: 2.5344\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 148s 304ms/step - loss: 2.4647\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 148s 303ms/step - loss: 2.4051\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "hidden_size = 1024\n",
    "model6 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model6.compile(loss=loss, optimizer=optimizer)\n",
    "history6 = model6.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-qatar",
   "metadata": {},
   "source": [
    "hidden_size를 감소 시켰을때보다 변화율은 크게 없지만 그래도 loss값이 증가함을 볼 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-panama",
   "metadata": {},
   "source": [
    "### (3) 그 외의 시도들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "demonstrated-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 69s 137ms/step - loss: 4.4768\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 68s 139ms/step - loss: 3.3127\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 68s 139ms/step - loss: 3.1057\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 68s 140ms/step - loss: 2.9909\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 68s 140ms/step - loss: 2.9162\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 68s 140ms/step - loss: 2.8430\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 68s 140ms/step - loss: 2.7738\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 68s 140ms/step - loss: 2.7236\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 68s 139ms/step - loss: 2.6630\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 68s 139ms/step - loss: 2.6199\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 512\n",
    "model7 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model7.compile(loss=loss, optimizer=optimizer)\n",
    "history7 = model7.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "relevant-democracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 69s 136ms/step - loss: 4.4924\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 67s 137ms/step - loss: 3.2911\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 67s 137ms/step - loss: 3.1434\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 67s 138ms/step - loss: 3.0350\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 67s 138ms/step - loss: 2.9495\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 67s 138ms/step - loss: 2.8756\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 67s 138ms/step - loss: 2.8195\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 67s 137ms/step - loss: 2.7545\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 67s 138ms/step - loss: 2.6976\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 67s 137ms/step - loss: 2.6479\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "hidden_size = 512\n",
    "model8= TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model8.compile(loss=loss, optimizer=optimizer)\n",
    "history8 = model8.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "consistent-reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 39s 75ms/step - loss: 4.8274\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 37s 75ms/step - loss: 3.4392\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 37s 75ms/step - loss: 3.3268\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 37s 75ms/step - loss: 3.2393\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 3.1567\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 3.0799\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 3.0268\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 2.9838\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 2.9436\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 37s 76ms/step - loss: 2.9025\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "hidden_size = 256\n",
    "model8= TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model8.compile(loss=loss, optimizer=optimizer)\n",
    "history8 = model8.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intensive-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 27s 50ms/step - loss: 5.1672\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.7942\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.4487\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.3244\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.2004\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.1176\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.0595\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 3.0139\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 2.9708\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 25s 50ms/step - loss: 2.9346\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "hidden_size = 128\n",
    "model10= TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model10.compile(loss=loss, optimizer=optimizer)\n",
    "history10 = model10.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "agricultural-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 41s 79ms/step - loss: 4.7558\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 39s 79ms/step - loss: 3.4441\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 3.3226\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 3.1945\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 3.0918\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 3.0191\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 2.9646\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 2.9160\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 2.8813\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 39s 80ms/step - loss: 2.8364\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "model11= TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model11.compile(loss=loss, optimizer=optimizer)\n",
    "history11 = model11.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sonic-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 29s 55ms/step - loss: 5.1376\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.5679\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.4268\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.3522\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.2601\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.1663\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.0966\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 3.0419\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 2.9873\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 27s 55ms/step - loss: 2.9439\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 128\n",
    "model12= TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model12.compile(loss=loss, optimizer=optimizer)\n",
    "history12=  model12.fit(dataset, epochs=10)"
   ]
  },
  {
   "attachments": {
    "s%20s%20s.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "crude-municipality",
   "metadata": {},
   "source": [
    "![s%20s%20s.png](attachment:s%20s%20s.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-posting",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "> - 이미지 처리와 다르게 자연어의 전처리 과정은 목적에 맞게 토큰화 및 정제, 정규화 과정을 거쳐야 한다.\n",
    "> - Embedding이란 사람의 언어(자연어)를 컴퓨터가 이해할 수 있게 수치화 해주는 과정을 뜻한다.\n",
    "> - buffer_size값을 src_input의 길이와 같게 혹은 더 크게 줘야한다. 값을 작게 줄 경우, 몇 부분을 놓치고 셔플 할 수 있다.\n",
    "> - embedding_size를 줄일수록 학습 속도가 느리다. hidden_size를 줄일수록 학습 속도가 빠르다.\n",
    "> - embedding_size를 줄일수록 loss의 값이 증가 했지만 서서히 증가하는 모습을 보였다.\n",
    "> - hidden_size를 줄일수록, embedding과 마찬가지고 loss가 증가했지만 상대적으로 많이 증가하는 모습을 보였다. loss에 영향이 더 큰 모습을 보였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-kennedy",
   "metadata": {},
   "source": [
    "## 자료 출처\n",
    " - 정제, 정규화, 토큰화 : <https://wikidocs.net/21698>\n",
    " - pad_sequences : https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    " - Embedding : https://simpling.tistory.com/1\n",
    " - LSTM : http://www.incodom.kr/LSTM\n",
    " - Dense : https://ssongnote.tistory.com/13\n",
    " - Hidden layer : https://www.techopedia.com/definition/33264/hidden-layer-neural-networks\n",
    " - imbedding size : https://algopoolja.tistory.com/34\n",
    " - hidden size : https://wikidocs.net/22886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-polymer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
